{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlY+XtV3+6okGMT1OVL2Sb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedml42/language-modeling/blob/main/notebooks/transformer-training-stability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_V7kVCUXa8rA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import LayerNorm, Linear, Dropout\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = {}\n",
        "norms = {}\n",
        "parameters = {}\n",
        "for i in range(1):\n",
        "  #transforms[f'transform_{i}'] = Linear(1, 1)\n",
        "  #parameters[f't_{i}'] = list(transforms[f'transform_{i}'].parameters())\n",
        "\n",
        "  norms[f'norm_{i}'] = LayerNorm(1)\n",
        "  parameters[f'norm_{i}'] = list(norms[f'norm_{i}'].parameters())\n",
        "\n",
        "#print(transforms)\n",
        "print(norms)\n",
        "print(parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01eDTukdUQjZ",
        "outputId": "74273250-7248-43d6-ff96-4646972787e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'norm_0': LayerNorm((1,), eps=1e-05, elementwise_affine=True)}\n",
            "{'norm_0': [Parameter containing:\n",
            "tensor([1.], requires_grad=True), Parameter containing:\n",
            "tensor([0.], requires_grad=True)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  x = torch.rand(1)\n",
        "  print('input:', x) \n",
        "  for i in range(1):\n",
        "    x = norms[f'norm_{i}'](x)\n",
        "\n",
        "  print('output:', x)\n",
        "  x.backward()\n",
        "  \n",
        "  for module, params in parameters.items():\n",
        "    print('Value', module, [x.grad for x in params])\n",
        "  \n",
        "  for module, params in parameters.items():\n",
        "    print('Grad', module, [x.grad for x in params])\n",
        "  print()\n",
        "\n",
        "  for module, params in parameters.items():\n",
        "    for p in params:\n",
        "      p.grad = torch.zeros(1)"
      ],
      "metadata": {
        "id": "bggFCzpRMS4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322ed718-6d49-428d-ce9e-3e4b9b965722"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([0.4537])\n",
            "output: tensor([-4.9060e-06], grad_fn=<NativeLayerNormBackward0>)\n",
            "Value norm_0 [tensor([-4.9060e-06]), tensor([1.])]\n",
            "Grad norm_0 [tensor([-4.9060e-06]), tensor([1.])]\n",
            "\n",
            "input: tensor([0.9097])\n",
            "output: tensor([-1.4391e-05], grad_fn=<NativeLayerNormBackward0>)\n",
            "Value norm_0 [tensor([-1.4391e-05]), tensor([1.])]\n",
            "Grad norm_0 [tensor([-1.4391e-05]), tensor([1.])]\n",
            "\n",
            "input: tensor([0.2251])\n",
            "output: tensor([3.7963e-06], grad_fn=<NativeLayerNormBackward0>)\n",
            "Value norm_0 [tensor([3.7963e-06]), tensor([1.])]\n",
            "Grad norm_0 [tensor([3.7963e-06]), tensor([1.])]\n",
            "\n",
            "input: tensor([0.4895])\n",
            "output: tensor([4.7208e-07], grad_fn=<NativeLayerNormBackward0>)\n",
            "Value norm_0 [tensor([4.7208e-07]), tensor([1.])]\n",
            "Grad norm_0 [tensor([4.7208e-07]), tensor([1.])]\n",
            "\n",
            "input: tensor([0.5125])\n",
            "output: tensor([-1.1589e-06], grad_fn=<NativeLayerNormBackward0>)\n",
            "Value norm_0 [tensor([-1.1589e-06]), tensor([1.])]\n",
            "Grad norm_0 [tensor([-1.1589e-06]), tensor([1.])]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  x = torch.rand(1)\n",
        "  print('input:', x) \n",
        "  for i in range(1):\n",
        "    x = norms[f'norm_{i}'](x)\n",
        "    print(list(parameters[f'transform_{i}'].parameters()))\n",
        "  \n",
        "  print('output:', x)\n",
        "  x.backward()\n",
        " \n",
        "  for module, params in parameters.items():\n",
        "    print(module)\n",
        "    print([x.grad for x in params])\n",
        "  print()"
      ],
      "metadata": {
        "id": "YPstjF1PcSE3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}